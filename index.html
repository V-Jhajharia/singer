
<html>
	<head>
		<style type="text/css">
			body{font-family: sans-serif;
			background-image: url("https://i.pinimg.com/originals/9d/e0/36/9de0364b3bf913e64daa65699c04821c.jpg")}
			p{display: inline-block;}
			img{display: block;}
			.container{width: 90%;position absolute;margin: auto;}
			.title{position: relative;width: 90%;margin: auto;text-align: center;font-weight: bold;font-size: 22px;padding: 1%;}
			.section{position: relative;width: 90%;margin: auto;padding: 2%;}
			.subsection{position: relative; width: 98%;text-align: justify;padding: 18px;}
			.heading{position: relative; width: 98%;text-align: left;font-size: 20px;font-weight: bold;}
			.text{width: 95%;font-size: 16px;text-align: justify;padding: 10px 0px 10px 0px;}
			.authors{position: relative;width: 80%;margin: auto;padding: 2%;font-style: italic;text-align: center;font-size: 16px;}
			.image{width: 95%;font-size: 14px;text-align: left;}
			
		</style>
	</head>
	<body>
		<div class="container">
			<div class="title">Singer Identification</div>

			<div class="authors">

				<!-- Start edit here  -->
				<p>Vaibhav Jhajharia, Roll No.: 150108040, Branch: EEE</p>; &nbsp; &nbsp;
				<p>Lavish K. Bansla, Roll No.: 150108019, Branch: EEE</p>; &nbsp; &nbsp;
				<p>Shashikesh Raushan, Roll No.: 150108035, Branch: EEE</p>; &nbsp; &nbsp;
				<p>Rohit Kumar, Roll No.: 150108030, Branch: EEE</p>; &nbsp; &nbsp;
				<!-- Stop edit here -->

			</div>


			<div class="section">
				<div class="heading">Abstract</div>
				<div class="text">

					<!-- Start edit here  -->
					&emsp;Singer identification is a difficult topic in music information retrieval because background instrumental music is included with singing voice which reduces performance of a system. One of the main disadvantages of the existing system is vocals and instrumental are separated manually and only vocals are used to build training model. But here we recognize a singer without separating instrumental and singing sounds using audio features like timbre coefficients, pitch class, mel frequency cepstral coefficients (MFCC), linear predictive coding (LPC) coefficients, and loudness of an audio signal.53 audio features (12 dimensional timbre audio feature vectors, 12 pitch classes, 13 MFCC coefficients, 13 LPC coefficients, and 3 loudness feature vector of an audio signal) are extracted from each segment. Dimension of extracted audio features is reduced using principal component analysis (PCA) method. Then we apply	Back propagation algorithm using neural network to train the singer classification model.				<!-- Stop edit here -->

				</div>
			</div>

			<div class="section">
				<div class="heading">1. Introduction</div>
				<div class="text">

					<!-- Start edit here  -->
					&emsp;Singing voice is one of the most important parameters in Indian songs. The singing voice of a singer is the element of a song which attracts the listeners. Singing is a continuous speech. Therefore speech and synthesis analysis techniques are not the same for singing voice. There is no efficient algorithm which works fine on speech identification and singing voice characterization together. So information on the singer’s voice is essential to organize, extract, and classify music collections . Sometimes, a viewer is interested to hear Indian songs based on their interest like favorite playback singer, actor, and actress. So there is a requirement to develop a system which provides the above features.
					<!-- Stop edit here -->

				</div>

				<div class="subsection">
					<div class="heading">1.1 Introduction to Problem</div>
					<div class="text">

						<!-- Start edit here  -->
						&emsp;The proposed system can identify singing voice and recognize a singer from audio songs. Significant accuracy can be achieved by extracting features of audio part from indian songs. One of the usefulness of this system is famous Indian playback singer’s songs can be identified from a big database. It can be useful to learn singer voice characteristics by listening to songs of different genres. It can be useful in categorizing unlabeled songs and copyright protection. These songs require information in different dimensions for efficient searching and indexing. Indian songs are marketed by its music, actor, and actress. So it is necessary to index Indian song using parameters like playback singers, actor, and actress for efficient search and retrieval.
						<!-- Stop edit here -->

					</div>
				</div>

				<div class="subsection">
					<div class="heading">1.2 Figure</div>
					<div class="image">

						<!-- Start edit here  -->
						
						singer/example.PNG
 
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.3 Literature Review</div>
					<div class="text">

						<!-- Start edit here  -->
						&emsp;A significant amount of research has been performed on speaker identification from digitized speech for applications such as verification of identity. These systems use features which are used in speech recognition and speaker recognition. Systems are trained on data without background noise and performance tend to degrade in noisy environments. They are trained on spoken data in which it produces poor result for the singing voice input. Mel frequency cepstral coefficients (MFCCs) are originally developed for automatic speech recognition applications and can be used for music modeling. Pitch and rhythm audio features are computed. MFCC feature vectors and artificial neural network classifier are used to identify playback singer from a database. Instrumental and singing sounds were not separated in the system. Music features are extracted for a musicological purpose using Echo Nest API. In , spectrogram is an effective parameter in time-frequency feature which is used as input classification. Several classification techniques are compared such as feed forward network and k-nearest neighbor. Energy function, zero crossing rate, and harmonic coefficients are used for singer identification. One of the drawbacks of the above system is training model is generated manually. Singer voice is separated manually by removing instrumental music from audio songs and it is used to build training model. In our proposed approach, training model is built automatically, not manually. Vocals and instrumental music are not separated out manually. Both are used to build training model. In other systems, only audio songs of singers are considered.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.4 Proposed Approach</div>
					<div class="text">

						<!-- Start edit here  -->
						&emsp;The abstract model of the proposed system for playback singer recognition using perceptual features and cepstral coefficients of an audio signal from Indian songs is shown in Fig. 1. It comprises of six building blocks: (1) song collection, (2) segmentation, (3) feature extraction, (4) dimension reduction, (5) model generation, and (6) singer recognition.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">1.5 Report Organization</div>
					<div class="text">

						<!-- Start edit here  -->
						<p>The report is organized as follows:<br/>

	 &emsp;1.Collection of N Indian audio songs of M singers.<br/>
 &emsp;2.Compute x1, x2, x3 ……x53 audio feature vectors for each segment of audio portion. Where x1-x12 timbre audio feature vectors, x13-x24 pitch class, x25-x27 loudness, x28- x40 MFCC feature vectors, and x41-x53 LPC coefficients. These features are stored in S1 structure. Size of S1 structure is S x 53, where S is total number of segments. Total number of segments depend on length of audio portion and audio feature.<br/>
 &emsp;3.Mean removal technique is applied on S1 structure and result is stored in S2 structure.<br/>
 &emsp;4.Principal component analysis method is used on S2 structure to compute eigenvalue, eigenvector using single value decomposition (SVD) technique. Score is obtained. Result is stored in score structure which is divided into two parts (training dataset (80 %) and testing dataset (20 %)).<br/>
&emsp;5.PSM is obtained using back propagation and AdaBoost.M2 algorithm using neural network. PSM is also obtained using KNN, GMM, and naïve Bayes classifier.<br/>
	&emsp;6.Compute probability of each song for M singers from test sample song dataset.<br/>
 	&emsp;7.Return a recognize singer name which contains maximum probability among Msingers.</p>
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading">2. Proposed Approach</div>
				<div class="text">

					<!-- Start edit here  -->
					<p>The approach used here for singer identification are:<br/>
						<br/>
1.Song collection<br/>
						
2.Segmentation</p>
<img src="https://static-content.springer.com/image/art%3A10.1186%2Fs13636-015-0062-9/MediaObjects/13636_2015_62_Fig2_HTML.gif"> 
<p>Fig. Segmentation process from Indian songs.</p>

<p>3.Feature extraction
      <br/>&emsp;a).Mel frequency cepstral coefficients:
           <br/> &emsp;&emsp;MFCC are the most useful coefficients which are used for speech recognition because of their ability to represent speech amplitude spectrum in a compact form. Figure below shows the process of creating MFCC features. Speech signal is divided into frames by applying a hamming windowing function at fixed intervals. Cepstral feature vectors are generated using each frame.
	<br/>&emsp;Steps involved in its calculation are as follows:
<br/>&emsp;&emsp; Preprocessing of the wav-file is the first step. It includes pre-emphasis, normalization and dc offset
removal.
<br/>&emsp;&emsp; The Fourier transform of a windowed and framed signal is calculated.
<br/>&emsp;&emsp; Triangular overlapping windows are used to map the power of the spectrum obtained above onto the
Mel scale.
<br/>&emsp;&emsp; Logs of the powers at each of the Mel frequencies are found.
<br/>&emsp;&emsp; Discrete cosine transform (DCT) of the list of Mel log powers is taken.
<br/>&emsp;&emsp; The MFCCs are the amplitude of the resulting spectrum. First thirteen coefficients are saved.
<br/>&emsp;Conversion from linear frequency scale to the Mel scale frequency Mf is achieved using the following equation
𝑚𝑓 = 2595 𝑙𝑜𝑔10(1 +
𝑓
700
) (1)
<br/>Where, f is frequency in hertz in linear scale.
	    </p>
	
<img src="https://static-content.springer.com/image/art%3A10.1186%2Fs13636-015-0062-9/MediaObjects/13636_2015_62_Fig4_HTML.gif"> 
  <p>Fig. The process of creating MFCC coefficients</p>

      
     <p>b).Linear predictive coding coefficients:<br/>
	     <be/>&emsp;LPC is a model for speech signal production based on the assumption that the speech signal is
produced by a very specific model [5].All LPC variants are based on the same simple model of an excitation
signal and a filter.LPC determines the coefficients of a forward linear predictor by minimizing the prediction
error in the least squares sense. It has applications in filter design and speech coding.
<br/>A closer inspection of this system shows that speech can be modeled as a pth order autoregressive process, where
the present sample, x(k) depends on the linear combination of past p samples added with a stochastic or random
component that represents noise. In other words, it is an all-pole FIR filter with Gaussian noise as input.
<br/>Where ai are the linear prediction coefficients (LPCs) and u(k), the process noise, is a zero-mean
Gaussian noise with variance σ2
u .
             <br/>&emsp;Linear predictive coding (LPC) can provide a very accurate spectral representation for speech sound. LPC coefficients are computed by using following equation:<br/>
X(n)=--a(2)X(n--1) -- a(3)X(n-2)……--a(P+1)X(n--P)
     <br/>where p is the order of the polynomial,a=[1 a(2)… A(P+1)].</p>
   <p>4. Dimension reduction:<br/>
&emsp;Principal component analysis method is used to compute principal component which reduces the dimension of extracted audio features. It retains as much as possible variance in the audio features. Principal components are extracted by a linear transformation to a new set of features which are uncorrelated and are ordered according to their importance. Principal component is computed using singular value decomposition algorithm.</p>
   <p>5. Model generation:<br/>
&emsp;In the proposed approach, playback singer model (PSM) is obtained using a different classification algorithm. The following models are generated: (1) Gaussian mixture model, (2) k-nearest neighbor model, (3) naïve Bayes classifier (NBC), (4) back propagation algorithm using neural network (BPNN), and (5) AdaBoost.M2 model.</p>
   <p>6. Singer recognition:<br/>
&emsp;In our proposed approach, singer recognition is carried out using trained models. Indian songs which are not used in trained models are given for testing using various classification algorithms which leads to recognition of a singer.</p>
					<!-- Stop edit here -->

				</div>
			</div>

			<div class="section">
				<div class="heading">3. Experiments &amp; Results</div>
				<div class="subsection">
					<div class="heading">3.1 Dataset Description</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">3.2 Discussion</div>
					<div class="text">

						<!-- Start edit here  -->
						<p>&emsp;The mel-frequency cepstralcoefficients (MFCCs) are the set of coefficients that gives the idea of spectral shape of sound. MFCCs are considered to be very effective in music analysis domain. A total of thirteen coefficients were considered for our work. Mirmfcc function is used to compute the value of MFCCs</p>
						<img src="http://ieeexplore.ieee.org/mediastore/IEEE/content/media/7836831/7853053/7853641/7853641-fig-3-large.gif">
						MFCCs graph for song sample of Kishore Kumar
						
						<!-- Stop edit here -->

					</div>
				</div>
			</div>

			<div class="section">
				<div class="heading">4. Conclusions</div>
				<div class="subsection">
					<div class="heading">4.1 Summary</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				</div>
				<div class="subsection">
					<div class="heading">4.2 Future Extensions</div>
					<div class="text">

						<!-- Start edit here  -->
						Write something here.
						<!-- Stop edit here -->

					</div>
				
				<div class="heading">5.References</div>
					<div class="text">

						<!-- Start edit here  -->
					<p>[1]. Salim, A., GowriSankar, “A novel method for Text-Independent speaker identification using MFCC
and GMM,” Proc. IEEE, Audio Language and Image Processing (ICALIP), Shanghai, Nov. 2010.
<br/>[2]. Ahmad Al Marashli, Dr. Oumayma Al Dakkak, “Automatic, Text-Independent, Speaker Identification
and Verification System Using Mel Cepstrum and ANN,” Information and Communication
Technologies: From Theory to Applications, 2008. ICTTA 3rd International Conference, 978-1-4244-
1751-3, Apr. 2008.
<br/>[3]. “Speaker recognition using Mel frequency Cepstral Coefficients (MFCC) and Vector quantization
(VQ) techniques,” Proc. IEEE, 978-1-4577-1326-2, Cholula, Puebla, Feb. 2012.
<br/>[4]. O Lartiloot, P Toiviainen and T Eerola, “A Matlab Toolbox for Music Information Retrieval,”
Springer, pp.261-268, 2008.
<br/>[5]. B.S Atal. Speech analysis and synthesis by linear prediction of the speech wave. Journal of the
Acoustical Society of America, 47(1):65, 1970.</p>

						<!-- Stop edit here -->

			</div>

		</div>
	</body>
</html>
